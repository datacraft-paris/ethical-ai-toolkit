{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "import dalex as dx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from copy import copy\n",
    "import re\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ...\n"
     ]
    }
   ],
   "source": [
    "print('loading data ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, target = dx.datasets.load_german(), \"risk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns=[target]),\n",
    "    df[target],\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X = df.drop(columns='risk')\n",
    "y = df.risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading models ...\n"
     ]
    }
   ],
   "source": [
    "print('loading models ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_bias(modeltype, biastype):\n",
    "\n",
    "    #BIAS PART\n",
    "    regexp_both = re.compile(r'^sex[mf]+ [0-9]{2}[\\+-]$')\n",
    "    regexp_sex = re.compile(r'^sex[mf]$')\n",
    "    regexp_age = re.compile(r'^[0-9]{2}[\\+-]$')\n",
    "    \n",
    "    # If both age and sex\n",
    "    if regexp_both.search(biastype):\n",
    "        protected = X.sex + '_' + np.where(X.age < int(biastype[5:7]), 'young', 'old')\n",
    "        mtype = 2\n",
    "        if biastype[0:4]==\"sexm\":\n",
    "            if biastype[7] == \"+\":\n",
    "                privileged = \"male_old\"\n",
    "            else:\n",
    "                privileged = \"male_young\"\n",
    "        else:\n",
    "            if biastype[7] == \"+\":\n",
    "                privileged = \"female_old\"\n",
    "            else:\n",
    "                privileged = \"female_young\"\n",
    "                \n",
    "    # If age only\n",
    "    elif regexp_age.search(biastype):\n",
    "        protected = np.where(X.age < int(biastype[0:2]), 'young', 'old')\n",
    "        mtype = 1\n",
    "        if biastype[2] == \"+\":\n",
    "            privileged = \"old\"\n",
    "        else:\n",
    "            privileged = \"young\"\n",
    "        \n",
    "    # If sex only\n",
    "    elif regexp_sex.search(biastype):\n",
    "        protected = X.sex\n",
    "        mtype = 0\n",
    "        if biastype==\"sexm\":\n",
    "            privileged = \"male\"\n",
    "        else:\n",
    "            privileged = \"female\"\n",
    "        \n",
    "    else:\n",
    "        print(\"ERROR in bias definition ! \")\n",
    "        return None\n",
    "        \n",
    "    # MODEL PART\n",
    "    model = \"m_\"+modeltype\n",
    "    if mtype==2: \n",
    "        model_rm = \"m_\"+modeltype+\"_a_s\"\n",
    "    if mtype==1: \n",
    "        model_rm = \"m_\"+modeltype+\"_a\"\n",
    "    if mtype==0: \n",
    "        model_rm = \"m_\"+modeltype+\"_s\"\n",
    "        \n",
    "    # OUTPUT\n",
    "    dict_informations = {\"model_name\": model, \"model_rm_name\": model_rm, \n",
    "                         \"protected\": protected, \"privileged\": privileged,\n",
    "                        }\n",
    "\n",
    "    print(\"Everything looks good, let's continue ! You can run the following cell ! \")\n",
    "    \n",
    "    return dict_informations\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "preprocessor_std = make_column_transformer(\n",
    "      (StandardScaler(), make_column_selector(dtype_include=np.number)),\n",
    "      (OneHotEncoder(), make_column_selector(dtype_include=object))\n",
    ")\n",
    "\n",
    "preprocessor_rm_sex = make_column_transformer(\n",
    "      (StandardScaler(), make_column_selector(dtype_include=np.number)),\n",
    "      (OneHotEncoder(), ['housing', 'saving_accounts', 'checking_account', 'purpose'])\n",
    ")\n",
    "\n",
    "preprocessor_rm_age = make_column_transformer(\n",
    "      (StandardScaler(), ['job', 'credit_amount', 'duration']),\n",
    "      (OneHotEncoder(), make_column_selector(dtype_include=object))\n",
    ")\n",
    "\n",
    "\n",
    "preprocessor_rm_age_sex = make_column_transformer(\n",
    "      (StandardScaler(), ['job', 'credit_amount', 'duration']),\n",
    "      (OneHotEncoder(), ['housing', 'saving_accounts', 'checking_account', 'purpose'])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('standardscaler',\n",
       "                                                  StandardScaler(),\n",
       "                                                  ['job', 'credit_amount',\n",
       "                                                   'duration']),\n",
       "                                                 ('onehotencoder',\n",
       "                                                  OneHotEncoder(),\n",
       "                                                  ['housing', 'saving_accounts',\n",
       "                                                   'checking_account',\n",
       "                                                   'purpose'])])),\n",
       "                ('classifier',\n",
       "                 GradientBoostingClassifier(learning_rate=1.0, max_depth=5,\n",
       "                                            random_state=123))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model & fir \n",
    "# Logistic regression\n",
    "m_logreg = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_std),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "m_logreg_s = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_rm_sex),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "m_logreg_a = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_rm_age),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "m_logreg_a_s = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_rm_age_sex),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "\n",
    "m_logreg.fit(X_train, y_train)\n",
    "m_logreg_s.fit(X_train, y_train)\n",
    "m_logreg_a.fit(X_train, y_train)\n",
    "m_logreg_a_s.fit(X_train, y_train)\n",
    "\n",
    "# Decision tree\n",
    "m_decisiontree = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_std),\n",
    "    ('classifier', DecisionTreeClassifier(max_depth=7, random_state=123))\n",
    "])\n",
    "\n",
    "m_decisiontree_s = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_rm_sex),\n",
    "    ('classifier', DecisionTreeClassifier(max_depth=7, random_state=123))\n",
    "])\n",
    "\n",
    "m_decisiontree_a = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_rm_age),\n",
    "    ('classifier', DecisionTreeClassifier(max_depth=7, random_state=123))\n",
    "])\n",
    "\n",
    "m_decisiontree_a_s = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_rm_age_sex),\n",
    "    ('classifier', DecisionTreeClassifier(max_depth=7, random_state=123))\n",
    "])\n",
    "\n",
    "\n",
    "m_decisiontree.fit(X_train, y_train)\n",
    "m_decisiontree_s.fit(X_train, y_train)\n",
    "m_decisiontree_a.fit(X_train, y_train)\n",
    "m_decisiontree_a_s.fit(X_train, y_train)\n",
    "\n",
    "# Random Forest\n",
    "m_randomforest = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_std),\n",
    "    ('classifier', RandomForestClassifier(random_state=123, max_depth=5))\n",
    "])\n",
    "\n",
    "m_randomforest_s = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_rm_sex),\n",
    "    ('classifier', RandomForestClassifier(random_state=123, max_depth=5))\n",
    "])\n",
    "\n",
    "m_randomforest_a = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_rm_age),\n",
    "    ('classifier', RandomForestClassifier(random_state=123, max_depth=5))\n",
    "])\n",
    "\n",
    "m_randomforest_a_s = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_rm_age_sex),\n",
    "    ('classifier', RandomForestClassifier(random_state=123, max_depth=5))\n",
    "])\n",
    "\n",
    "m_randomforest.fit(X_train, y_train)\n",
    "m_randomforest_s.fit(X_train, y_train)\n",
    "m_randomforest_a.fit(X_train, y_train)\n",
    "m_randomforest_a_s.fit(X_train, y_train)\n",
    "\n",
    "# XGBoost\n",
    "\n",
    "m_gbtree = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_std),\n",
    "    ('classifier', GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=5, random_state=123))\n",
    "])\n",
    "\n",
    "m_gbtree_s = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_rm_sex),\n",
    "    ('classifier', GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=5, random_state=123))\n",
    "])\n",
    "\n",
    "m_gbtree_a = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_rm_age),\n",
    "    ('classifier', GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=5, random_state=123))\n",
    "])\n",
    "\n",
    "m_gbtree_a_s = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_rm_age_sex),\n",
    "    ('classifier', GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=5, random_state=123))\n",
    "])\n",
    "\n",
    "\n",
    "m_gbtree.fit(X_train, y_train)\n",
    "m_gbtree_s.fit(X_train, y_train)\n",
    "m_gbtree_a.fit(X_train, y_train)\n",
    "m_gbtree_a_s.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loading pipes ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re sampling\n",
    "def resampling_model(model, model_explainer, protected, type_resampling=\"uniform\"):\n",
    "    \n",
    "    # Select observations for re sampling\n",
    "    if type_resampling == \"preferential\":\n",
    "        indices = dx.fairness.resample(protected, y, type = 'preferential', # different type \n",
    "                                                    probs = explainer.y_hat, # requires probabilities \n",
    "                                                    verbose = False)\n",
    "    else:\n",
    "        indices = dx.fairness.resample(protected, y, verbose = False)\n",
    "    \n",
    "    \n",
    "    # create new model bjects\n",
    "    new_model = copy(model)\n",
    "    \n",
    "    # re-train models\n",
    "    new_model.fit(X.iloc[indices, :], y[indices])\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re weighting\n",
    "def reweighting_model(model):\n",
    "    weights = dx.fairness.reweight(protected, y, verbose = False)\n",
    "    \n",
    "    model_weighted = model\n",
    "\n",
    "    kwargs = {model_weighted.steps[-1][0] + '__sample_weight': weights}\n",
    "    \n",
    "    model_weighted.fit(X,y, **kwargs)\n",
    "    \n",
    "    return model_weighted\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_radar_group(fobject, title=None, metrics=[\"TPR\", \"ACC\", \"PPV\", \"FPR\", \"STP\"]):\n",
    "    \n",
    "    if metrics == 'all':\n",
    "        metrics = fobject.metric_scores.columns\n",
    "    \n",
    "    # Prevent crash for wrong metric name\n",
    "    metrics_common = [x for x in metrics if x in fobject.metric_scores]\n",
    "    \n",
    "    df = fobject.metric_scores[metrics_common].stack().reset_index()\n",
    "    df.columns = ['Group', 'Fairness Metric', 'Value']\n",
    "    \n",
    "    fig = px.line_polar(df,\n",
    "                        r=\"Value\",\n",
    "                        theta='Fairness Metric',\n",
    "                        color=\"Group\",\n",
    "                        line_close=True,\n",
    "                        hover_name='Group')\n",
    "\n",
    "    if title is None:\n",
    "        title = 'Radar Plot by Group'\n",
    "        \n",
    "    fig.update_layout(title=title, polar=dict(radialaxis=dict(tickangle=0, nticks=6, range=[0, 1])))\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_explainers_fairness_objects(parameters_dict, epsilon=0.8, theta=0.02, runsubpart=None):\n",
    "    \n",
    "    protected = parameters_dict['protected']\n",
    "    privileged = parameters_dict['privileged']\n",
    "    \n",
    "    # Explainer & Fairness object of base model (selected by user)\n",
    "    explainer_base = dx.Explainer(eval(parameters_dict['model_name']), X, y, verbose=False)\n",
    "    print(\"-\")\n",
    "    fairness_object_base = explainer_base.model_fairness(protected = protected, privileged = privileged, verbose=False)\n",
    "    fairness_object.fairness_check(epsilon = epsilon)\n",
    "\n",
    "    # Mitigation 1: Remove sensitive columns\n",
    "    print('---')\n",
    "    explainer_rm = dx.Explainer(eval(parameters_dict['model_rm_name']), X, y, verbose=False)\n",
    "    fobject_rm = explainer_rm.model_fairness(protected = protected, privileged = privileged, verbose=False, label='base_remove_columns')\n",
    "    fobject_rm.fairness_check(epsilon = epsilon)\n",
    "    print('---')\n",
    "    # Mitigation 2: Resampling data \n",
    "    model_p = resampling_model(eval(parameters_dict['model_name']), explainer_base, protected, type_resampling=\"preferential\")\n",
    "    model_u = resampling_model(eval(parameters_dict['model_name']), explainer_base, protected, type_resampling=\"uniform\")\n",
    "\n",
    "    explainer_p = dx.Explainer(model_p, X, y, verbose = False)\n",
    "    explainer_u = dx.Explainer(model_u, X, y, verbose = False)\n",
    "\n",
    "    fobject_p = explainer_p.model_fairness(protected, privileged, verbose=False, label='base_preferential_resampling')\n",
    "    fobject_u = explainer_u.model_fairness(protected, privileged, verbose=False, label='base_uniform_resampling')\n",
    "\n",
    "    # Mitigation 3: Reweighting data\n",
    "    model_w = reweighting_model(eval(parameters_dict['model_name']))\n",
    "\n",
    "    explainer_w = dx.Explainer(model_w, X, y, verbose = False)\n",
    "\n",
    "    fobject_w = explainer_w.model_fairness(protected, privileged, verbose=False, label='base_reweighted')\n",
    "    \n",
    "    # Mitigation 4: Roc-pivot switcher\n",
    "    explainer_roc = dx.Explainer(eval(parameters_dict['model_name']), X, y, verbose=False)\n",
    "    explainer_roc = dx.fairness.roc_pivot(explainer_roc, protected, privileged, theta = theta, verbose = False)\n",
    "    fobject_roc = explainer_roc.model_fairness(protected, privileged, verbose=False, label='base_roc-pivot')\n",
    "\n",
    "    return (explainer_base, fairness_object_base), (explainer_rm, fobject_rm), (explainer_p, fobject_p),(explainer_u, fobject_u), (explainer_w, fobject_w), (explainer_roc, fobject_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(metricslist, graphic, explainers_fairness, parameters_dict, epsilon = 0.8):\n",
    "    \n",
    "    # BIT OF CHECKS & PREPROCESSING\n",
    "    metrics = [x for x in metricslist if x in [\"TPR\", \"TNR\", \"PPV\", \"NPV\", \"FNR\", \"FPR\", \"FDR\", \"FOR\", \"ACC\", \"STP\"]]\n",
    "    if len(metrics) < 1:\n",
    "        return (\"Please, select correct metrics (List of possiblities is above)\")\n",
    "    \n",
    "    if graphic not in ['Default', 'Radar', 'Stack']:\n",
    "        print (\"Graphic type is incorrect, the default one will be used\")\n",
    "        graphic == \"Default\"\n",
    "    \n",
    "    # 1 - STATUS OF THE SELECTED MODEL\n",
    "    \n",
    "    print(\"Let's check the fairness performance of the selected model according to the specific populations declared:\")\n",
    "    \n",
    "    print(\"\"\"  This is a default graph produced by the dalex library. It does not take into account selection made for METRICS_LIST and GRAPHIC.\n",
    "    \n",
    "    \"\"\")\n",
    "    protected = parameters_dict['protected']\n",
    "    privileged = parameters_dict['privileged']\n",
    "    \n",
    "    # Explainer & Fairness object of base model (selected by user)\n",
    "    explainer_base = dx.Explainer(eval(parameters_dict['model_name']), X, y, verbose=False)\n",
    "    fairness_object_base = explainer_base.model_fairness(protected = protected, privileged = privileged, verbose=False)\n",
    "    fairness_object.fairness_check(epsilon = epsilon)\n",
    "    \n",
    "    explainers_fairness[0][1].plot()\n",
    "    \n",
    "    print(\"\"\"\\033[1m Doing nothing. Is that so bad, really ?\\033[0m\n",
    "\n",
    "  • If all bars are in the green area, then according to your criteria your model is not biased. However, if you set a threshold for the age, are you sure that moving it a little bit (± 1 to 5 years) will not return a biased result ? Try it to be sure !\n",
    "\n",
    "  • If a bias has been detected, have a look below to see how you can mitigate it !\n",
    "  \"\"\")\n",
    "    \n",
    "    # 2 - NAIVE APPROACH\n",
    "    print(\"\\033[1m\" + \"Trying to mitigate a bias\" + \"\\033[0m\")\n",
    "    print(\"\\033[1m\" + \"  Option 1: Remove the sensitive variable\" + \"\\033[0m\")\n",
    "    print(\"\"\"  \n",
    "    This is a default graph produced by the dalex library. It does not take into account selection made for METRICS_LIST and GRAPHIC.\n",
    "    \n",
    "    \"\"\")\n",
    "    explainers_fairness[1][1].plot([explainers_fairness[0][1]])\n",
    "    \n",
    "    print(\"\"\"How did you model evolved regarding to your fairness metrics ? Is it better without the column ? \n",
    "\n",
    "  • If yes, you're lucky this kind of naive preprocessing used to be useless most of the time. Usually the protected and biased variable is correlated with others explanatory variables and then removing it do not helps to unbias your model ! \n",
    "\n",
    "  • If no, well that kind of normal, let's see more appropriate ways to deal with biased models.\n",
    "  \n",
    "  \"\"\")\n",
    "    \n",
    "    # 3 - RESAMPLING\n",
    "    print(\"\\033[1m\" + \"Trying to mitigate a bias\" + \"\\033[0m\")\n",
    "    print(\"\\033[1m\" + \"  Option 2: Resampling training data\" + \"\\033[0m\")\n",
    "    \n",
    "    print(\"\"\"  \n",
    "    Did you look at the distribution of the biased variable ? Maybe some values of the variable are under-represented or over-represented. Resampling more equally training data would help to mitigate bias due to this king of issue.\n",
    "    \n",
    "    Let's compare the effect of this method to the default model:\n",
    "    \"\"\")\n",
    "    if graphic != 'Default':\n",
    "        explainers_fairness[0][1].plot([explainers_fairness[2][1], explainers_fairness[3][1]], \n",
    "                                      type=graphic, \n",
    "                                      metrics=metrics)\n",
    "    else:\n",
    "        explainers_fairness[0][1].plot([explainers_fairness[2][1], explainers_fairness[3][1]])\n",
    "        \n",
    "    print(\"\"\"So far, this solution may have resolved the unfairness issue. If not let's see another possibility !\n",
    "    \n",
    "    \"\"\")    \n",
    "    \n",
    "        # 3 - REWEIGHTING\n",
    "    print(\"\\033[1m\" + \"Trying to mitigate a bias\" + \"\\033[0m\")\n",
    "    print(\"\\033[1m\" + \"  Option 3: Reweighting observations\" + \"\\033[0m\")\n",
    "    \n",
    "    print(\"\"\"  \n",
    "    The reweighting algorithm looks at the protected attribute and on the real label. Then, it calculates the probability of assigning favorable label (y=1) assuming the protected attribute and y are independent. Of course, if there is bias, they will be statistically dependent. Then, the algorithm divides calculated theoretical probability by true, empirical probability of this event. That is how weight is created.\n",
    "    \n",
    "    Let's compare the effect of this method to the default model:\n",
    "    \"\"\")\n",
    "    \n",
    "    if graphic != 'Default':\n",
    "        explainers_fairness[0][1].plot([explainers_fairness[4][1]], \n",
    "                                      type=graphic, \n",
    "                                      metrics=metrics)\n",
    "    else:\n",
    "        explainers_fairness[0][1].plot([explainers_fairness[4][1]])\n",
    "        \n",
    "    print(\"\"\"So far, this solution may have resolved the unfairness issue. If not let's see another possibility !\n",
    "    \n",
    "    \"\"\")    \n",
    "    \n",
    "        # 4 - ROC\n",
    "    print(\"\\033[1m\" + \"Trying to mitigate a bias\" + \"\\033[0m\")\n",
    "    print(\"\\033[1m\" + \"  Option 4: The ROC-Pivot method (Postprocessing)\" + \"\\033[0m\")\n",
    "    \n",
    "    print(\"\"\"  \n",
    "    This method of mitigation aims to change predictions for items close to the decision frontier.\n",
    "It switches labels if an observation is from the unprivileged group and on the left (wrong side) of the cutoff. Note that It can also switches labels if an observation is from the privileged group and on the right of the cutoff.\n",
    "    \n",
    "    Let's compare the effect of this method to the default model:\n",
    "    \"\"\")\n",
    "    \n",
    "    if graphic != 'Default':\n",
    "        explainers_fairness[0][1].plot([explainers_fairness[5][1]], \n",
    "                                      type=graphic, \n",
    "                                      metrics=metrics)\n",
    "    else:\n",
    "        explainers_fairness[0][1].plot([explainers_fairness[5][1]])\n",
    "        \n",
    "    print(\"\"\"This 3 previous methods are implemented by the library dalex to mitigate bias.\n",
    "    \n",
    "    Let's see below all solutions in 1 graphic:\"\"\")    \n",
    "    \n",
    "    \n",
    "    if graphic != 'Default':\n",
    "        explainers_fairness[0][1].plot([explainers_fairness[2][1], explainers_fairness[3][1], \n",
    "                                        explainers_fairness[4][1], explainers_fairness[5][1]], \n",
    "                                      type=graphic, \n",
    "                                      metrics=metrics)\n",
    "    else:\n",
    "        explainers_fairness[0][1].plot([explainers_fairness[2][1], explainers_fairness[3][1], \n",
    "                                        explainers_fairness[4][1], explainers_fairness[5][1]])\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    return \"tada\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_groups_fairness(selected_model, explainers_fairness, metricslist):\n",
    "    \n",
    "    metrics = [x for x in metricslist if x in [\"TPR\", \"TNR\", \"PPV\", \"NPV\", \"FNR\", \"FPR\", \"FDR\", \"FOR\", \"ACC\", \"STP\"]]\n",
    "    \n",
    "    \n",
    "    if selected_model == \"base\":\n",
    "        plot_radar_group(explainers_fairness[0][1], \n",
    "                         title=None, \n",
    "                         metrics=metrics)\n",
    "    \n",
    "    elif selected_model == \"sampling_p\":\n",
    "        plot_radar_group(explainers_fairness[2][1], \n",
    "                         title=None, \n",
    "                         metrics=metrics)\n",
    "    \n",
    "    elif selected_model == \"sampling_u\":\n",
    "        plot_radar_group(explainers_fairness[3][1], \n",
    "                         title=None, \n",
    "                         metrics=metrics)\n",
    "    \n",
    "    elif selected_model == \"weights\":\n",
    "        plot_radar_group(explainers_fairness[4][1], \n",
    "                         title=None, \n",
    "                         metrics=metrics)\n",
    "    \n",
    "    elif selected_model == \"roc-pivot\":\n",
    "        plot_radar_group(explainers_fairness[5][1], \n",
    "                         title=None, \n",
    "                         metrics=metrics)\n",
    "\n",
    "    elif selected_model == \"remove\":\n",
    "        plot_radar_group(explainers_fairness[1][1], \n",
    "                         title=None, \n",
    "                         metrics=metrics)\n",
    "  \n",
    "    else:\n",
    "        print(\"model not recognize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"all loaded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
